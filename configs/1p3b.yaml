# Copyright 2023 Cerebras Systems.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# 1.3B parameter model trained for 20tpp. Configuration roughly matches
# the Cerebras-GPT 1.3B model except for the choice of dataset

# model params
trainer:
  init:
    # device: 'CPU'
    seed: 1
    backend: 
      backend_type: 'CPU'
      cluster_config:
        max_act_per_csx: 1
        num_csx: 1        
        mount_dirs:
         - /path/to/code/location
         - /path/to/data/location        
        python_paths:
         - /path/to/code/location
        mgmt_address: 172.31.63.134:9000
    model:
      width: 2048
      depth: 24
      max_position_embeddings: 2048
      heads: 16
      vocab_size: 50257
      dropout: 0.0
      bias: True
      init_std: 0.01
    model_dir: 'out_dir/'
    loop:
      num_steps: 24334
    checkpoint:
      steps: 2000
    precision:
      precision_opt_level: 1
      max_gradient_norm: 1.0
    optimizer:
      AdamW:
        learning_rate: 2.0e-4
        weight_decay: 0.1
        eps: 1.0e-9
    schedulers:
      SequentialLR:
        schedulers:
          - LinearLR:
              initial_learning_rate: 0.0
              end_learning_rate: 2.0e-4
              total_iters: 346
          - CosineDecayLR:
              initial_learning_rate: 2.0e-4
              end_learning_rate: 6.0e-5
              total_iters: 23988
    loggers:
      - TensorBoardLogger: {}     
  fit:
    train_dataloader:
      data_dir: /path/to/train/dataset # replace this with the correct absolute path before running
      sequence_length: 2048
      batch_size: 528      
      seed: 1    
  validate:
    val_dataloader:
      data_dir: /path/to/val/dataset # replace this with the correct absolute path before running
      sequence_length: 2048
      batch_size: 528      
      seed: 1
    ckpt_path: /path/to/checkpoint/file
